# LLaMA-7B-LORA
CUDA_VISIBLE_DEVICES=0 python finetune.py   --base_model 'yahma/llama-7b-hf'   --data_path 'math_10k.json'   --output_dir './trained_models/llama-7b-lora-math/'   --batch_size 16  --micro_batch_size 4   --num_epochs 3   --learning_rate 3e-4   --cutoff_len 256   --val_set_size 0 --eval_step 80 --save_step 80  --adapter_name lora --target_modules '["q_proj", "k_proj", "v_proj", "up_proj", "down_proj"]' --lora_r 32 --lora_alpha 64

# LLaMA-7B-prefix
CUDA_VISIBLE_DEVICES=0 python finetune.py   --base_model 'yahma/llama-7b-hf'   --data_path 'math_10k.json'   --output_dir './trained_models/llama-7b-prefix-math-vt10/'   --batch_size 8  --micro_batch_size 4   --num_epochs 5   --learning_rate 3e-2   --cutoff_len 256   --val_set_size 120 --eval_step 10 --save_step 10  --adapter_name prefix-tuning --num_virtual_tokens 10

# LLaMA-7B-series
CUDA_VISIBLE_DEVICES=0 python finetune.py   --base_model 'yahma/llama-7b-hf'   --data_path 'math_10k.json'   --output_dir './trained_models/llama-7b-bottleneck-math-attn/'   --batch_size 16  --micro_batch_size 4   --num_epochs 3   --learning_rate 3e-4   --cutoff_len 256   --val_set_size 120 --eval_step 80 --save_step 80  --adapter_name bottleneck --load_8bit --target_modules '["up_proj", "gate_proj"]'

# LLaMA-7B-Parallel
CUDA_VISIBLE_DEVICES=0 python finetune.py   --base_model 'yahma/llama-7b-hf'   --data_path 'math_10k.json'   --output_dir './trained_models/llama-7b-parallel-math/'   --batch_size 16  --micro_batch_size 4   --num_epochs 3   --learning_rate 3e-4   --cutoff_len 256   --val_set_size 0 --eval_step 80 --save_step 80  --adapter_name bottleneck --use_parallel_adapter --target_modules '["up_proj", "down_proj"]' 

#For LLaMA-13B models, we use `--use_gradient_checkpointing` to save memory

# BLOOMZ-7B-LORA
CUDA_VISIBLE_DEVICES=0 python finetune.py   --base_model 'bigscience/bloomz-7b1'   --data_path 'math_10k.json'   --output_dir './trained_models/bloomz-7b-lora-math-all-r32/'   --batch_size 16  --micro_batch_size 4   --num_epochs 3   --learning_rate 3e-4   --cutoff_len 256   --val_set_size 0 --eval_step 80 --save_step 80  --adapter_name lora --load_8bit --target_modules '["query_key_value", "dense_4h_to_h", "dense_h_to_4h"]' --lora_r 32 --lora_alpha 64 

# BLOOMZ-7B-series
CUDA_VISIBLE_DEVICES=0 python finetune.py   --base_model 'bigscience/bloomz-7b1'   --data_path 'math_10k.json'   --output_dir './trained_models/bloomz-7b-bottleneck-math-mlp-bs256/'   --batch_size 16  --micro_batch_size 4   --num_epochs 3   --learning_rate 3e-4   --cutoff_len 256   --val_set_size 0 --eval_step 80 --save_step 80  --adapter_name bottleneck --target_modules '["dense_4h_to_h"]'


# BLOOMZ-7B-Parallel
CUDA_VISIBLE_DEVICES=0 python finetune.py   --base_model 'bigscience/bloomz-7b1'   --data_path 'math_10k.json'   --output_dir './trained_models/bloomz-7b-parallel-math-mlp-bs256/'   --batch_size 16  --micro_batch_size 4   --num_epochs 3   --learning_rate 3e-4   --cutoff_len 256   --val_set_size 0 --eval_step 80 --save_step 80  --adapter_name bottleneck --use_parallel_adapter --load_8bit --target_modules '["dense_4h_to_h", "dense_h_to_4h"]' 

# GPT-6B-LORA
CUDA_VISIBLE_DEVICES=0 python finetune.py   --base_model 'EleutherAI/gpt-j-6b'   --data_path 'math_10k.json'   --output_dir './trained_models/gptj-6b-lora-math-all-r32/'   --batch_size 16  --micro_batch_size 4   --num_epochs 3   --learning_rate 3e-4   --cutoff_len 256   --val_set_size 0 --eval_step 80 --save_step 80  --adapter_name lora --target_modules '["q_proj", "k_proj", "v_proj", "fc_in", "fc_out"]' --lora_r 32 --lora_alpha 64


# GPT-6B-Series
CUDA_VISIBLE_DEVICES=0 python finetune.py   --base_model 'EleutherAI/gpt-j-6b'   --data_path 'math_10k.json'   --output_dir './trained_models/gptj-6b-bottleneck-math-mlp-bs256/'   --batch_size 16  --micro_batch_size 4   --num_epochs 3   --learning_rate 3e-4   --cutoff_len 256   --val_set_size 0 --eval_step 80 --save_step 80  --adapter_name bottleneck --target_modules '["fc_out"]'


# GPT-6B-Parallel
CUDA_VISIBLE_DEVICES=0 python finetune.py   --base_model 'EleutherAI/gpt-j-6b'   --data_path 'math_10k.json'   --output_dir './trained_models/gptj-6b-parallel-math-mlp-bs256/'   --batch_size 16  --micro_batch_size 4   --num_epochs 3   --learning_rate 3e-4   --cutoff_len 256   --val_set_size 0 --eval_step 80 --save_step 80  --adapter_name bottleneck --use_parallel_adapter --load_8bit --target_modules '["fc_in", "fc_out"]'
